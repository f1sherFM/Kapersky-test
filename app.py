import pandas as pd
import requests
from bs4 import BeautifulSoup
import difflib
import re
import os
from datetime import datetime
import json
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from tenacity import retry, stop_after_attempt, wait_exponential
from tqdm import tqdm
from html_report import generate_html_report

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
INPUT_CSV = r".\Test_check.csv"
OUTPUT_DIR = "results"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# --- –§—É–Ω–∫—Ü–∏–∏ ---

def ensure_directory_exists():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def extract_text_from_url(URL):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL"""
    try:
        response = requests.get(URL, headers=HEADERS, timeout=20)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for tag in ['script', 'style', 'nav', 'footer', 'iframe', 'img', 'button']:
            for element in soup.find_all(tag):
                element.decompose()

        # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        article_body = (
            soup.find('article') or
            soup.find(class_=re.compile(r'(post-body|article-text|entry-content|articleBody)', re.I)) or
            soup.find(itemprop='articleBody') or
            soup.find('div', class_='article__text') or
            soup.find('body')
        )

        text = article_body.get_text(separator='\n', strip=True) if article_body else soup.get_text(separator='\n', strip=True)
        return re.sub(r'\n{2,}', '\n\n', text).strip()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {URL}: {e}")
        return f"–û—à–∏–±–∫–∞: {str(e)}"

def compare_texts(original_text, lib_text):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏"""

    def normalize(text):
        return re.sub(r'\s+', ' ', text.strip().lower())

    matcher = difflib.SequenceMatcher(None, normalize(original_text), normalize(lib_text))
    similarity = matcher.ratio() * 100  # –ü—Ä–æ—Ü–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞

    original_lines = [line.strip() for line in original_text.split('\n') if line.strip()]
    lib_lines = [line.strip() for line in lib_text.split('\n') if line.strip()]

    missing = [line for line in original_lines if normalize(line) not in normalize(lib_text)]
    extra = [line for line in lib_lines if normalize(line) not in normalize(original_text)]

    return {
        'similarity': round(similarity, 2),
        'original_length': len(original_text),
        'lib_length': len(lib_text),
        'missing_lines_count': len(missing),
        'extra_lines_count': len(extra),
        'example_missing': list(set(missing))[:3],
        'example_extra': list(set(extra))[:3]
    }

def generate_ai_analysis(comparison_data):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""
    analysis = []
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞
    similarity = comparison_data['similarity']
    if similarity > 80:
        analysis.append("‚úÖ –¢–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–±–æ–ª–µ–µ 80%). –û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ.")
    elif similarity > 50:
        analysis.append("‚ö†Ô∏è –¢–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç —É–º–µ—Ä–µ–Ω–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (50-80%). –ò–º–µ—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è.")
    else:
        analysis.append("‚ùå –¢–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç –Ω–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–º–µ–Ω–µ–µ 50%). –ö–æ–Ω—Ç–µ–Ω—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è.")

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
    missing_count = comparison_data['missing_lines_count']
    if missing_count > 0:
        analysis.append(f"\nüîç –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—Ç—Ä–æ–∫: {missing_count}")
        analysis.append("–ù–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏:")
        for example in comparison_data['example_missing'][:3]:
            analysis.append(f"- {example[:150]}{'...' if len(example) > 150 else ''}")
    
    # –ê–Ω–∞–ª–∏–∑ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
    extra_count = comparison_data['extra_lines_count']
    if extra_count > 0:
        analysis.append(f"\nüîç –î–æ–±–∞–≤–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: {extra_count}")
        analysis.append("–ù–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è:")
        for example in comparison_data['example_extra'][:3]:
            analysis.append(f"- {example[:150]}{'...' if len(example) > 150 else ''}")

    # –û–±—â–∏–π –≤—ã–≤–æ–¥
    analysis.append("\nüìå –í—ã–≤–æ–¥:")
    if missing_count > extra_count:
        analysis.append("–û—Å–Ω–æ–≤–Ω–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ - –ø—Ä–æ–ø—É—Å–∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.")
    elif extra_count > missing_count:
        analysis.append("–û—Å–Ω–æ–≤–Ω–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ.")
    else:
        analysis.append("–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç –∫–∞–∫ –ø—Ä–æ–ø—É—Å–∫–∏, —Ç–∞–∫ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")

    return "\n".join(analysis)

def save_results(results):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON –∏ CSV"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_path = os.path.join(OUTPUT_DIR, f'results_{timestamp}.json')
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å AI-–∞–Ω–∞–ª–∏–∑–æ–º
    full_report = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source_csv": INPUT_CSV,
            "total_articles": len(results),
            "successful": len([r for r in results if r['status'] == 'success']),
            "failed": len([r for r in results if r['status'] == 'error'])
        },
        "articles": []
    }
    
    for result in results:
        article_data = {
            "url": result['url'],
            "status": result['status'],
            "timestamp": datetime.now().isoformat()
        }
        
        if result['status'] == 'success':
            article_data.update({
                "similarity": result['similarity'],
                "length_analysis": {
                    "original": result['original_length'],
                    "lib": result['lib_length'],
                    "difference": abs(result['original_length'] - result['lib_length'])
                },
                "content_differences": {
                    "missing_lines": result['missing_lines_count'],
                    "extra_lines": result['extra_lines_count'],
                    "example_missing": result['example_missing'],
                    "example_extra": result['example_extra']
                },
                "ai_analysis": generate_ai_analysis(result)
            })
        else:
            article_data['error'] = result['error']
        
        full_report["articles"].append(article_data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(full_report, f, ensure_ascii=False, indent=2, default=str)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    csv_path = os.path.join(OUTPUT_DIR, f'results_{timestamp}.csv')
    pd.DataFrame(results).to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\n- –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç: {json_path}\n- –¢–∞–±–ª–∏—Ü–∞ –¥–∞–Ω–Ω—ã—Ö: {csv_path}")

def generate_comprehensive_report(results):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Excel"""
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment, PatternFill
    from openpyxl.utils import get_column_letter
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = os.path.join(OUTPUT_DIR, f'comprehensive_report_{timestamp}.xlsx')
    
    # –°–æ–∑–¥–∞–µ–º –∫–Ω–∏–≥—É Excel
    wb = Workbook()
    
    # === –õ–∏—Å—Ç 1: –°–≤–æ–¥–∫–∞ ===
    ws_summary = wb.active
    ws_summary.title = "–°–≤–æ–¥–∫–∞"
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    headers = [
        "URL", "–°—Ö–æ–¥—Å—Ç–≤–æ (%)", "–°—Ç–∞—Ç—É—Å",
        "–î–ª–∏–Ω–∞ —ç—Ç–∞–ª–æ–Ω–∞", "–î–ª–∏–Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ", "–†–∞–∑–Ω–∏—Ü–∞",
        "–ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—Ç—Ä–æ–∫", "–î–æ–±–∞–≤–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫", "–û—Ü–µ–Ω–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤",
        "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (—Å–æ–≤–µ—Ç—ã –ø–æ —É–ª—É—á—à–µ–Ω–∏—é)"  # –ù–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü
    ]
    
    for col_num, header in enumerate(headers, 1):
        ws_summary.cell(row=1, column=col_num, value=header).font = Font(bold=True)
    
    # –î–∞–Ω–Ω—ã–µ
    for row_num, result in enumerate(results, 2):
        ws_summary.cell(row=row_num, column=1, value=result['url'])
        
        if result['status'] == 'success':
            ws_summary.cell(row=row_num, column=2, value=result['similarity'])
            ws_summary.cell(row=row_num, column=4, value=result['original_length'])
            ws_summary.cell(row=row_num, column=5, value=result['lib_length'])
            ws_summary.cell(row=row_num, column=6, value=abs(result['original_length'] - result['lib_length']))
            ws_summary.cell(row=row_num, column=7, value=result['missing_lines_count'])
            ws_summary.cell(row=row_num, column=8, value=result['extra_lines_count'])
            
            # –û—Ü–µ–Ω–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤
            missing_importance = "–ù–∏–∑–∫–∞—è"
            if result['missing_lines_count'] > 10:
                missing_importance = "–°—Ä–µ–¥–Ω—è—è"
            if result['missing_lines_count'] > 30 or any(len(ex) > 100 for ex in result['example_missing']):
                missing_importance = "–í—ã—Å–æ–∫–∞—è"
            ws_summary.cell(row=row_num, column=9, value=missing_importance)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è (–Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü)
            comments = []
            
            # –°–æ–≤–µ—Ç—ã –ø–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É
            if result['missing_lines_count'] > 0:
                if any(re.search(r'(–∞–¥—Ä–µ—Å|–∫–æ–Ω—Ç–∞–∫—Ç|—Ç–µ–ª–µ—Ñ–æ–Ω)', ex, re.I) for ex in result['example_missing']):
                    comments.append("–î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
                if any(re.search(r'(–≤—ã–≤–æ–¥|–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–∏—Ç–æ–≥)', ex, re.I) for ex in result['example_missing']):
                    comments.append("–£–ª—É—á—à–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –≤—ã–≤–æ–¥–æ–≤.")
                if result['missing_lines_count'] > 15:
                    comments.append("–†–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")
            
            # –°–æ–≤–µ—Ç—ã –ø–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É
            if result['extra_lines_count'] > 0:
                if any(re.search(r'(—Ä–µ–∫–ª–∞–º|–ø—Ä–æ–º–æ|–∞–∫—Ü–∏)', ex, re.I) for ex in result['example_extra']):
                    comments.append("–î–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")
                if result['extra_lines_count'] > 10:
                    comments.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤.")
            
            # –û–±—â–∏–µ —Å–æ–≤–µ—Ç—ã
            if result['similarity'] < 50:
                comments.append("–¢—Ä–µ–±—É–µ—Ç—Å—è –≥–ª—É–±–æ–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
            elif result['similarity'] < 70:
                comments.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∞–≤–∏–ª –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.")
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –æ—Å–æ–±—ã—Ö –ø—Ä–æ–±–ª–µ–º
            if not comments and result['similarity'] > 80:
                comments.append("–ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–æ—Ä–æ—à–µ–µ. –ú–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤.")
            
            ws_summary.cell(row=row_num, column=10, value="\n".join(comments) if comments else "–ù–µ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π")
        else:
            ws_summary.cell(row=row_num, column=2, value="N/A")
            error = result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
            
            # –°–æ–≤–µ—Ç—ã –ø–æ –æ—à–∏–±–∫–∞–º
            error_advice = {
                'timeout': "–£–≤–µ–ª–∏—á–∏—Ç—å —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏.",
                '404': "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å URL.",
                '403': "–î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å User-Agent."
            }
            
            advice = "–û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä–µ—Å—É—Ä—Å–∞."
            for err_key, err_advice in error_advice.items():
                if err_key in error.lower():
                    advice = err_advice
                    break
            
            ws_summary.cell(row=row_num, column=10, value=f"–û—à–∏–±–∫–∞: {error}\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {advice}")
        
        ws_summary.cell(row=row_num, column=3, value="–£—Å–ø–µ—Ö" if result['status'] == 'success' else "–û—à–∏–±–∫–∞")
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    for column in ws_summary.columns:
        max_length = 0
        column = [cell for cell in column]
        for cell in column:
            try:
                if len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = (max_length + 2) * 1.2
        ws_summary.column_dimensions[get_column_letter(column[0].column)].width = adjusted_width
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ç–æ–ª–±—Ü–∞ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
    for row in ws_summary.iter_rows(min_row=2, max_col=10, max_row=len(results)+1):
        row[9].alignment = Alignment(wrapText=True, vertical='top')  # 10-–π —Å—Ç–æ–ª–±–µ—Ü (–∏–Ω–¥–µ–∫—Å 9)
    
    # === –õ–∏—Å—Ç 2: –î–µ—Ç–∞–ª–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è ===
    ws_details = wb.create_sheet("–î–µ—Ç–∞–ª–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    headers = [
        "URL", "–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫", "–ü—Ä–∏–º–µ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫", 
        "–ê–Ω–∞–ª–∏–∑ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
    ]
    
    for col_num, header in enumerate(headers, 1):
        ws_details.cell(row=1, column=col_num, value=header).font = Font(bold=True)
    
    # –î–∞–Ω–Ω—ã–µ
    for row_num, result in enumerate(results, 2):
        ws_details.cell(row=row_num, column=1, value=result['url'])
        
        if result['status'] == 'success':
            # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            missing_examples = "\n".join([f"- {ex[:200]}{'...' if len(ex) > 200 else ''}" 
                                         for ex in result['example_missing']])
            ws_details.cell(row=row_num, column=2, value=missing_examples)
            
            # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            extra_examples = "\n".join([f"- {ex[:200]}{'...' if len(ex) > 200 else ''}" 
                                      for ex in result['example_extra']])
            ws_details.cell(row=row_num, column=3, value=extra_examples)
            
            # –ê–Ω–∞–ª–∏–∑ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
            analysis = []
            if result['missing_lines_count'] > 0:
                analysis.append(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ {result['missing_lines_count']} —Å—Ç—Ä–æ–∫. ")
                if any(re.search(r'(–∞–¥—Ä–µ—Å|–∫–æ–Ω—Ç–∞–∫—Ç|—Ç–µ–ª–µ—Ñ–æ–Ω|–ø–æ—á—Ç–∞)', ex, re.I) for ex in result['example_missing']):
                    analysis.append("–ü—Ä–æ–ø—É—â–µ–Ω—ã –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. ")
                if any(re.search(r'(–≤—ã–≤–æ–¥|–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–∏—Ç–æ–≥)', ex, re.I) for ex in result['example_missing']):
                    analysis.append("–ü—Ä–æ–ø—É—â–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã. ")
            
            if result['extra_lines_count'] > 0:
                analysis.append(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {result['extra_lines_count']} —Å—Ç—Ä–æ–∫. ")
                if any(re.search(r'(–ø–æ–ª–∏—Ç–∏–∫|—ç–∫–æ–Ω–æ–º–∏–∫|–∫–æ–Ω—Ñ–ª–∏–∫—Ç)', ex, re.I) for ex in result['example_extra']):
                    analysis.append("–î–æ–±–∞–≤–ª–µ–Ω –ø–æ–ª–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. ")
            
            ws_details.cell(row=row_num, column=4, value=" ".join(analysis) if analysis else "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è")
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = []
            if result['similarity'] < 50:
                recommendations.append("–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –Ω–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ.")
            elif result['similarity'] < 80:
                recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞.")
            
            if result['missing_lines_count'] > result['extra_lines_count']:
                recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")
            elif result['extra_lines_count'] > result['missing_lines_count']:
                recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")
            
            ws_details.cell(row=row_num, column=5, value=" ".join(recommendations) if recommendations else "–ü—Ä–∏–µ–º–ª–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        else:
            ws_details.cell(row=row_num, column=2, value=result['error'])
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Ç–µ–∫—Å—Ç–∞
    for row in ws_details.iter_rows():
        for cell in row:
            cell.alignment = Alignment(wrapText=True, vertical='top')
    
    # === –õ–∏—Å—Ç 3: –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ ===
    ws_analytics = wb.create_sheet("–ê–Ω–∞–ª–∏—Ç–∏–∫–∞")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = [
        ["–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π", len(results)],
        ["–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ", len([r for r in results if r['status'] == 'success'])],
        ["–û—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏", len([r for r in results if r['status'] == 'error'])],
        ["–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", np.mean([r['similarity'] for r in results if r['status'] == 'success']) if any(r['status'] == 'success' for r in results) else "N/A"],
        ["–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", max([r['similarity'] for r in results if r['status'] == 'success']) if any(r['status'] == 'success' for r in results) else "N/A"],
        ["–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", min([r['similarity'] for r in results if r['status'] == 'success']) if any(r['status'] == 'success' for r in results) else "N/A"]
    ]
    
    for row_num, stat in enumerate(stats, 1):
        ws_analytics.cell(row=row_num, column=1, value=stat[0]).font = Font(bold=True)
        ws_analytics.cell(row=row_num, column=2, value=stat[1])
    
    # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
    conclusion = [
        "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:",
        "",
        "1. –û–±—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:",
        f"- {'–í—ã—Å–æ–∫–∞—è' if len([r for r in results if r['status'] == 'success' and r['similarity'] > 80]) / len(results) > 0.7 else '–°—Ä–µ–¥–Ω—è—è' if len([r for r in results if r['status'] == 'success' and r['similarity'] > 50]) / len(results) > 0.7 else '–ù–∏–∑–∫–∞—è'} —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
        "",
        "2. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:",
        "- –ü—Ä–æ–ø—É—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞" if any(r['status'] == 'success' and r['missing_lines_count'] > 10 for r in results) else "",
        "- –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞" if any(r['status'] == 'success' and r['extra_lines_count'] > 10 for r in results) else "",
        "",
        "3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:",
        "- –£—Ç–æ—á–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤",
        "- –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ–∫–∞–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö URL",
        "",
        "4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:",
        "- –í–Ω–µ–¥—Ä–∏—Ç—å ML-–º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
        "- –î–æ–±–∞–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"
    ]
    
    for row_num, line in enumerate(conclusion, len(stats) + 2):
        ws_analytics.cell(row=row_num, column=1, value=line)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    wb.save(report_path)
    print(f"\nüìä –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")


def generate_improvement_tips(article):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–æ–≤–µ—Ç—ã –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
    tips = []
    
    if article['similarity'] < 50:
        tips.append("‚Ä¢ –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ—Ä—å–µ–∑–Ω–∞—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    elif article['similarity'] < 70:
        tips.append("‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª –∏–∑–≤–ª–µ—á–µ–Ω–∏—è")
    
    if article['missing_lines_count'] > 10:
        tips.append("‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤")
    
    if article['extra_lines_count'] > 5:
        tips.append("‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤")
    
    if any('—Ä–µ–∫–ª–∞–º' in ex.lower() for ex in article['example_extra']):
        tips.append("‚Ä¢ –í–Ω–µ–¥—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    if not tips:
        tips.append("‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ. –ú–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    return "<br>".join(tips)

def main():
    ensure_directory_exists()
    results = []

    print("=== –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ ===")
    print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞: {INPUT_CSV}")

    try:
        df_input = pd.read_csv(INPUT_CSV, on_bad_lines='skip', sep=None, engine='python', encoding='utf-8-sig')
        df_input.columns = df_input.columns.str.strip()
        
        if not all(col.lower() in [c.lower() for c in df_input.columns] for col in ['URL', 'lib_text']):
            print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏!")
            return

        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df_input)} –∑–∞–ø–∏—Å–µ–π. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...\n")

        for idx, row in tqdm(df_input.iterrows(), total=len(df_input), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π"):
            url = str(row['URL']).strip()
            lib_text = str(row['lib_text']).strip()

            if not lib_text:
                tqdm.write(f"‚ö†Ô∏è [{idx+1}] lib_text –ø—É—Å—Ç–æ–π. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                continue

            try:
                original_text = extract_text_from_url(url)
                
                if original_text.startswith("–û—à–∏–±–∫–∞"):
                    results.append({
                        'url': url,
                        'status': 'error',
                        'error': original_text
                    })
                    continue

                comparison = compare_texts(original_text, lib_text)
                results.append({
                    'url': url,
                    'similarity': comparison['similarity'],
                    'original_length': comparison['original_length'],
                    'lib_length': comparison['lib_length'],
                    'missing_lines_count': comparison['missing_lines_count'],
                    'extra_lines_count': comparison['extra_lines_count'],
                    'example_missing': comparison['example_missing'],
                    'example_extra': comparison['example_extra'],
                    'status': 'success'
                })

            except Exception as e:
                results.append({
                    'url': url,
                    'status': 'error',
                    'error': str(e)
                })
                tqdm.write(f"‚ùå [{idx+1}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {str(e)}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_results(results)
        generate_comprehensive_report(results)
        generate_html_report(results, OUTPUT_DIR)

    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
        return

    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df_input)} –∑–∞–ø–∏—Å–µ–π. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...\n")

    for idx, row in df_input.iterrows():
        url = str(row['URL']).strip()      # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã!
        lib_text = str(row['lib_text']).strip()

        print(f"[{idx+1}/{len(df_input)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {url}")

        if not lib_text:
            print("‚ö†Ô∏è lib_text –ø—É—Å—Ç–æ–π. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...\n")
            continue

        original_text = extract_text_from_url(url)

        if original_text.startswith("–û—à–∏–±–∫–∞"):
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {original_text}\n")
            results.append({
                'url': url,
                'status': 'error',
                'error': original_text
            })
            continue

        comparison = compare_texts(original_text, lib_text)

        result = {
            'url': url,
            'similarity': comparison['similarity'],
            'original_length': comparison['original_length'],
            'lib_length': comparison['lib_length'],
            'missing_lines_count': comparison['missing_lines_count'],
            'extra_lines_count': comparison['extra_lines_count'],
            'example_missing': " | ".join(comparison['example_missing']),
            'example_extra': " | ".join(comparison['example_extra']),
            'status': 'success'
        }

        results.append(result)
        print(f"‚úÖ –°—Ö–æ–¥—Å—Ç–≤–æ: {comparison['similarity']}%\n")


if __name__ == "__main__":
    main()